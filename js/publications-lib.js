const publicationsList = [
  // {
  //   id: 0,
  //   date: 'March 15, 2021',
  //   link: 'publications/publication.html',
  //   slug: 'ceam-the-effectiveness-of-cyclic',
  //   url: 'publications/publication.html#ceam-the-effectiveness-of-cyclic',
  //   title: 'CEAM: The Effectiveness of Cyclic and Ephemeral Attention Models of User Behavior on Social Platforms',
  //   description: '<p><b>Abstract</b></p><p>To improve the user experience as well as business outcomes, social platforms aim to predict user behavior. To this end, recurrent models are often used to predict a user’s next behavior based on their most recent behavior. However, people have habits and routines, making it plausible to predict their behavior from more than just their most recent activity. Our work focuses on the interplay between ephemeral and cyclical components of user behaviors. By utilizing user activity data from social platform Snapchat, we uncover cyclic and ephemeral usage patterns on a per user-level. Based on our findings, we imbued recurrent models with awareness: we augment an RNN with a cyclic module to complement traditionalRNNs that model ephemeral behaviors and allow a flexible weighting of the two for the prediction task. We conducted extensive experiments to evaluate our model’s performance on four user behavior prediction tasks on the Snapchat platform.We achieve improved results on each task compared against existing methods, using this simple, but important insight in user behavior: Both cyclical and ephemeral components matter. We show that in some situations and for some people, ephemeral components may be more helpful for predicting behavior, while for others and in other situations, cyclical components may carry more weight.</p>',
  //   authors: 'Farhan Chowdhury, Yozen Liu, Nick Vincent, Koustuv Saha, Leo Neves, Neil Shah, Maarten Bos',
  //   eventID: 0,
  //   researchArea: 'Computational Social Science',
  //   PDFLink: '',
  //   metaTitle: 'CEAM: The Effectiveness of Cyclic and Ephemeral Attention Models of User Behavior on Social Platforms - Snap Research',
  //   metaDescription: '',
  //   bgImage: ''
  // },
  // {
  //   id: 1,
  //   date: 'March 15, 2021',
  //   link: 'publications/publication.html',
  //   slug: 'online-communication-shifts-in-the-midst-of-the-covid-19-pandemic-a-case',
  //   url: 'publications/publication.html#online-communication-shifts-in-the-midst-of-the-covid-19-pandemic-a-case',
  //   title: 'Online Communication Shifts in the Midst of the Covid-19 Pandemic: A Case Study on Snapchat',
  //   description: '<p><b>Abstract</b></p><p>The Covid-19 pandemic has created large shifts in how people stay connected with each other in lieu of social distancing and isolation measures. More and more, individuals have turned to online communications as a necessary replacement for in-person interaction. Despite this, the research community has little understanding of how online communications have been influenced by the offline impacts of Covid-19. Our work touches upon this topic. Specifically, we study research questions around the impact of Covid-19 on online public and private sharing propensity, its influence on online communication homophily, and correlations between online communication and offline case severity in the United States. To do so, we study the usage patterns of 79 million US-based users on Snapchat, a large, leading mobile multimedia-driven social sharing platform. Our findings suggest that Covid-19 has increased propensity to privately communicate with friends, while decreasing propensity to publicly share content when users are out-and-about. Moreover, online communications have observed a marked decrease in baseline homophily across locations, ages and genders, with relative increases in cross-group communications. Finally, we observe that increased offline positive Covid-19 case severity in US states is associated with widening gaps between across-state and within-state communication increases after the onset of Covid-19, as well as marked declines in public sharing. We hope our findings drive further interest and work on online communication changes during pandemics and other extended times of crisis.</p>',
  //   authors: 'Qi Yang, Weinan Wang, Lucas Pierce, Rajan Vaish, Xiaolin Shi, Neil Shah',
  //   eventID: 0,
  //   researchArea: 'Computational Social Science, Data Science',
  //   PDFLink: '',
  //   metaTitle: 'Online Communication Shifts in the Midst of the Covid-19 Pandemic: A Case  Study on Snapchat - Snap Research',
  //   metaDescription: '',
  //   bgImage: ''
  // },
  // {
  //   id: 2,
  //   date: 'February 16, 2021',
  //   link: 'publications/publication.html',
  //   slug: 'significant-otter-understanding-the-role-of-biosignals-in-communication',
  //   url: 'publications/publication.html#significant-otter-understanding-the-role-of-biosignals-in-communication',
  //   title: 'Significant Otter: Understanding the Role of Biosignals in Communication',
  //   description: '<p><b>Abstract</b></p><p>With the growing ubiquity of wearable devices, sensed physiological responses provide new means to connect with others. While recent research demonstrates the expressive potential for biosignals, the value of sharing these personal data remains unclear. To understand their role in communication, we created Significant Otter, an Apple Watch/iPhone app that enables romantic partners to share and respond to each other’s biosignals in the form of animated otter avatars. In a one-month study with 20 couples, participants used Significant Otter with biosignals sensing OFF and ON. We found that while sensing OFF enabled couples to keep in touch, sensing ON enabled easier and more authentic communication that fostered social connection. However, the addition of biosignals introduced concerns about autonomy and agency over the messages they sent. We discuss design implications and future directions for communication systems that recommend messages based on biosignals.</p>',
  //   authors: 'Fannie Liu, Chunjong Park, Yu Jiang Tham, Tsung-Yu Tsai, Laura Dabbish, Geoff Kaufman, Andrés Monroy-Hernández',
  //   eventID: 1,
  //   researchArea: 'Human Computer Interaction',
  //   PDFLink: 'https://assets.ctfassets.net/btheynltg5cn/eOL9X92uuV2MjAFYZRgYX/a3ee211a9baf1ec472de66dfd360f028/2102.08235.pdf',
  //   metaTitle: 'Significant Otter: Understanding the Role of Biosignals in Communication - Snap Research',
  //   metaDescription: '',
  //   bgImage: 'https://images.ctfassets.net/btheynltg5cn/1sZ3K3mOk9WmgdEORIWYcA/1a26da31f46bf51d1124ef802e17e915/Screen_Shot_2021-02-20_at_2.48.12_AM.png?w=400'
  // },
  // {
  //   id: 3,
  //   date: 'February 08, 2021',
  //   link: 'publications/publication.html',
  //   slug: 'graph-neural-networks-for-friend-ranking-in-large-scale-social-platforms',
  //   url: 'publications/publication.html#graph-neural-networks-for-friend-ranking-in-large-scale-social-platforms',
  //   title: 'Graph Neural Networks for Friend Ranking in Large-scale Social Platforms',
  //   description: '<p><b>Abstract</b></p><p>Graph Neural Networks (GNNs) have recently enabled substantial advances in graph learning. Despite their rich representational capacity, GNNs remain under-explored for large-scale social modeling applications. One such industrially ubiquitous application is friend suggestion: recommending users other candidate users to befriend, to improve user connectivity, retention and engagement. However, modeling such user-user interactions on large-scale social platforms poses unique challenges: such graphs often have heavy-tailed degree distributions, where a significant fraction of users are inactive and have limited structural and engagement information. Moreover, users interact with different functionalities, communicate with diverse groups, and have multifaceted interaction patterns. We study the application of GNNs for friend suggestion, providing the first investigation of GNN design for this task, to our knowledge. To leverage the rich knowledge of in-platform actions, we formulate friend suggestion as multi-faceted friend ranking with multi-modal user features and link communication features. We design a neural architecture GraFRank to learn expressive user representations from multiple feature modalities and user-user interactions. Specifically, GraFRank employs modality-specific neighbor aggregators and cross-modality attentions to learn multi-faceted user representations. We conduct experiments on two multi-million user datasets from Snapchat, a leading mobile social platform, where GraFRank outperforms several state-of-the-art approaches on candidate retrieval (by 30% MRR) and ranking (by 20% MRR) tasks. Moreover, our qualitative analysis indicates notable gains for critical populations of less-active and low-degree users.</p>',
  //   authors: 'Aravind Sankar, Yozen Liu, Jun Yu, Neil Shah',
  //   eventID: 1,
  //   researchArea: 'Computational Social Science',
  //   PDFLink: 'https://assets.ctfassets.net/btheynltg5cn/53nweIQ8YcXmh9uRxAssqi/e43c35ef60dd613fe25689ca55e35ac9/GrafRank.WWW.21.pdf',
  //   metaTitle: 'Graph Neural Networks for Friend Ranking in Large-scale Social Platforms - Snap Research',
  //   metaDescription: '',
  //   bgImage: ''
  // },
  // {
  //   id: 4,
  //   date: 'January 04, 2021',
  //   link: 'publications/publication.html',
  //   slug: 'advertiming-matters-examining-user-ad-consumption-for-effective-ad',
  //   url: 'publications/publication.html#advertiming-matters-examining-user-ad-consumption-for-effective-ad',
  //   title: 'AdverTiming Matters: Examining User Ad Consumption for Effective Ad Allocations on Social Media',
  //   description: '<p>Showing ads delivers revenue for online content distributors, but ad exposure can compromise user experience and cause user fatigue and frustration. Correctly balancing ads with other content is imperative. Currently, ad allocation relies primarily on demographics and inferred user interests, which are treated as static features and can be privacy-intrusive. This paper uses person-centric and momentary context features to understand optimal ad-timing. In a quasi-experimental study on a three-month longitudinal dataset of 100K Snapchat users, we find ad timing influences ad effectiveness. We draw insights on the relationship between ad effectiveness and momentary behaviors such as duration, interactivity, and interaction diversity. We simulate ad reallocation, finding that our study-driven insights lead to greater value for the platform. This work advances our understanding of ad consumption and bears implications for designing responsible ad allocation systems, improving both user and platform outcomes. We discuss privacy-preserving components and ethical implications of our work.</p>',
  //   authors: 'Koustuv Saha, Yozen Liu, Nicholas Vincent, Farhan Asif Chowdhury, Leonardo Neves, Neil Shah, Maarten Bos',
  //   eventID: 2,
  //   researchArea: 'Computational Social Science',
  //   PDFLink: 'https://assets.ctfassets.net/btheynltg5cn/w1aqHgJ3zsegGpJUYKxbN/323c07d286a10d875a58c12d348ffad3/CHI21_AdverTiming_DoubleColumn.pdf',
  //   metaTitle: 'AdverTiming Matters: Examining User Ad Consumption for Effective Ad Allocations on Social Media - Snap Research',
  //   metaDescription: 'Showing ads delivers revenue for online content distributors, but ad exposure can compromise user experience and cause user fatigue and frustration. Correctly balancing ads with other content is imperative. Currently, ad allocation relies primarily on demographics and inferred user interests, which are treated as static features and can be privacy-intrusive. This paper uses person-centric and momentary context features to understand optimal ad-timing. In a quasi-experimental study on a three-month longitudinal dataset of 100K Snapchat users, we find ad timing influences ad effectiveness. We draw insights on the relationship between ad effectiveness and momentary behaviors such as duration, interactivity, and interaction diversity. We simulate ad reallocation, finding that our study-driven insights lead to greater value for the platform. This work advances our understanding of ad consumption and bears implications for designing responsible ad allocation systems, improving both user and platform outcomes. We discuss privacy-preserving components and ethical implications of our work.',
  //   bgImage: ''
  // },
  // {
  //   id: 5,
  //   date: 'December 05, 2020',
  //   link: 'publications/publication.html',
  //   slug: 'fairod-fairness-aware-outlier-detection',
  //   url: 'publications/publication.html#fairod-fairness-aware-outlier-detection',
  //   title: 'FairOD: Fairness-aware Outlier Detection',
  //   description: '<p>Fairness and Outlier Detection (OD) are closely related, as it is exactly the goal of OD to spot rare, minority samples in a given population. When being a minority (as defined by protected variables, e.g. race/ethnicity/sex/age) does not reflect positive-class membership (e.g. criminal/fraud), however, OD produces unjust outcomes. Surprisingly, fairness-aware OD has been almost untouched in prior work, as fair machine learning literature mainly focus on supervised settings. Our work aims to bridge this gap. Specifically, we develop desiderata capturing well-motivated fairness criteria for OD, and systematically formalize the fair OD problem. Further, guided by our desiderata, we propose FairOD, a fairness-aware outlier detector, which has the following, desirable properties: FairOD (1) does not employ disparate treatment at test time, (2) aims to flag equal proportions of samples from all groups (i.e. obtain group fairness, via statistical parity), and (3) strives to flag truly high-risk fraction of samples within each group. Extensive experiments on a diverse set of synthetic and real world datasets show that FairOD produces outcomes that are fair with respect to protected variables, while performing comparable to (and in some cases, even better than) fairness-agnostic detectors in terms of detection performance.</p>',
  //   authors: 'Shubhranshu Shekhar, Neil Shah, Leman Akoglu',
  //   eventID: 3,
  //   researchArea: 'Computational Social Science',
  //   PDFLink: 'https://assets.ctfassets.net/btheynltg5cn/4bSVQh0NCd9vuemQ9Euk8a/259be56e72ba2c0e70522f459c755861/2012.03063.pdf',
  //   metaTitle: 'FairOD: Fairness-aware Outlier Detection - Snap Research',
  //   metaDescription: 'Fairness and Outlier Detection (OD) are closely related, as it is exactly the goal of OD to spot rare, minority samples in a given population. When being a minority (as defined by protected variables, e.g. race/ethnicity/sex/age) does not reflect positive-class membership (e.g. criminal/fraud), however, OD produces unjust outcomes. Surprisingly, fairness-aware OD has been almost untouched in prior work, as fair machine learning literature mainly focus on supervised settings. Our work aims to bridge this gap. Specifically, we develop desiderata capturing well-motivated fairness criteria for OD, and systematically formalize the fair OD problem. Further, guided by our desiderata, we propose FairOD, a fairness-aware outlier detector, which has the following, desirable properties: FairOD (1) does not employ disparate treatment at test time, (2) aims to flag equal proportions of samples from all groups (i.e. obtain group fairness, via statistical parity), and (3) strives to flag truly high-risk fraction of samples within each group. Extensive experiments on a diverse set of synthetic and real world datasets show that FairOD produces outcomes that are fair with respect to protected variables, while performing comparable to (and in some cases, even better than) fairness-agnostic detectors in terms of detection performance.',
  //   bgImage: ''
  // },
  // {
  //   id: 6,
  //   date: 'December 02, 2020',
  //   link: 'publications/publication.html',
  //   slug: 'data-augmentation-for-graph-neural-networks',
  //   url: 'publications/publication.html#data-augmentation-for-graph-neural-networks',
  //   title: 'Data Augmentation for Graph Neural Networks',
  //   description: '<p><b>Abstract</b></p><p>Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This is largely due to the complex, non-Euclidean structure of graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language have no analogs for graphs. Our work studies graph data augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classification. We discuss practical and theoretical motivations, considerations and strategies for graph data augmentation. Our work shows that neural edge predictors can effectively encode classhomophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main contribution introduces the GAUG graph data augmentation framework, which leverages these insights to improve performance in GNN-based node classification via edge prediction. Extensive experiments on multiple benchmarks show that augmentation via GAUG improves performance across GNN architectures and datasets.</p>',
  //   authors: 'Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, Neil Shah',
  //   eventID: 4,
  //   researchArea: 'Computational Social Science',
  //   PDFLink: 'https://assets.ctfassets.net/btheynltg5cn/6U9l1ptDcrGoUPBOrDrChh/23ddcd2a0ab783ab47936196eeb15e2a/2006.06830.pdf',
  //   metaTitle: 'Data Augmentation for Graph Neural Networks - Snap Research',
  //   metaDescription: '',
  //   bgImage: ''
  // },
  // {
  //   id: 7,
  //   date: 'November 16, 2020',
  //   link: 'publications/publication.html',
  //   slug: 'tweet-eval-unified-benchmark-and-comparative-evaluation-for-tweet',
  //   url: 'publications/publication.html#tweet-eval-unified-benchmark-and-comparative-evaluation-for-tweet',
  //   title: 'TWEET EVAL: Unified Benchmark and Comparative Evaluation for Tweet Classification',
  //   description: '<p><b>Abstract</b>The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domainspecific data. In this paper, we propose a new evaluation framework (TWEETEVAL) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pretrained generic language models, and continue training them on Twitter corpora.</p>',
  //   authors: 'Francesco Barbieri, Jose Camacho-Collados, Leonardo Neves, Luis Espinosa-Anke†',
  //   eventID: 5,
  //   researchArea: 'Computational Social Science, Natural Language Processing',
  //   PDFLink: 'https://assets.ctfassets.net/btheynltg5cn/3mKDgDM0vE6hMvPZaisEZZ/aef05f083605760a1efc7e39e67c5d27/2020.findings-emnlp.148.pdf',
  //   metaTitle: 'TWEET EVAL: Unified Benchmark and Comparative Evaluation for Tweet Classification - Snap Research',
  //   metaDescription: '',
  //   bgImage: ''
  // },
  // {
  //   id: 8,
  //   date: 'November 02, 2020',
  //   link: 'publications/publication.html',
  //   slug: 'the-devil-is-in-the-details-evaluating-limitations-of-transformer-based',
  //   url: 'publications/publication.html#the-devil-is-in-the-details-evaluating-limitations-of-transformer-based',
  //   title: 'The Devil is in the Details: Evaluating Limitations of Transformer-based Methods for Granular Tasks',
  //   description: '<p><b>Abstract</b>Contextual embeddings derived from transformer-based neural language models have shown state-of-the-art performance for various tasks such as question answering, sentiment analysis, and textual similarity in recent years. Extensive work shows how accurately such models can represent abstract, semantic information present in text. In this expository work, we explore a tangent direction and analyze such models’ performance on tasks that require a more granular level of representation. We focus on the problem of textual similarity from two perspectives: matching documents on a granular level (requiring embeddings to capture fine-grained attributes in the text), and an abstract level (requiring embeddings to capture overall textual semantics).</p><p>We empirically demonstrate, across two datasets from different domains, that despite high performance in abstract document matching as expected, contextual embeddings are consistently (and at times, vastly) outperformed by simple baselines like TF-IDF for more granular tasks. We then propose a simple but effective method to incorporate TF-IDF into models that use contextual embeddings, achieving relative improvements of up to 36% on granular tasks.</p>',
  //   authors: 'Brihi Joshi, Neil Shah, Francesco Barbieri, Leonardo Neves',
  //   eventID: 6,
  //   researchArea: 'Computational Social Science, Natural Language Processing',
  //   PDFLink: 'https://assets.ctfassets.net/btheynltg5cn/WUgIBbb5JQ4yXsZTLi2ay/25d747d901068f12be951a9d4c4095b8/2011.01196.pdf',
  //   metaTitle: 'The Devil is in the Details: Evaluating Limitations of Transformer-based Methods for Granular Tasks - Snap Research',
  //   metaDescription: '',
  //   bgImage: ''
  // },
  // {
  //   id: 9,
  //   date: 'September 28, 2020',
  //   link: 'publications/publication.html',
  //   slug: 'a-good-image-generator-is-what-you-need-for-high-resolution-video-synthesis',
  //   url: 'publications/publication.html#a-good-image-generator-is-what-you-need-for-high-resolution-video-synthesis',
  //   title: 'A Good Image Generator Is What You Need for High-Resolution Video Synthesis',
  //   description: '<p><b>ABSTRACT</b></p><p>Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. </p><p>Code will be released at https://github.com/snap-research/MoCoGAN-HD.</p>',
  //   authors: 'Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov',
  //   eventID: 7,
  //   researchArea: 'Creative Vision',
  //   PDFLink: 'https://downloads.ctfassets.net/btheynltg5cn/5bv0CSn2Gz2No3YFNRB8cW/6f770978b86b6d86c11629447bc7ba19/pdf',
  //   metaTitle: 'A Good Image Generator Is What You Need for High-Resolution Video Synthesis - Snap Research',
  //   metaDescription: '',
  //   bgImage: ''
  // },
  {
    id: 1,
    date: "January 01, 2016",
    title: "L0-Sparse Subspace Clustering",
    description: `<p><b>Abstract: </b>Subspace clustering methods with sparsity prior, such as Sparse Subspace Clustering (SSC), are effective in partitioning the data that lie in a union of subspaces. Most of those methods require certain assumptions, e.g. independence or disjointness, on the subspaces. These assumptions are not guaranteed to hold in practice and they limit the application of existing sparse subspace clustering methods. In this paper, we propose <i>ℓ0</i>-induced sparse subspace clustering (ℓ0-SSC). In contrast to the required assumptions, such as independence or disjointness, on subspaces for most existing sparse subspace clustering methods, we prove that subspace-sparse representation, a key element in subspace clustering, can be obtained by ℓ0-SSC for arbitrary distinct underlying subspaces almost surely under the mild i.i.d. assumption on the data generation. We also present the “no free lunch” theorem that obtaining the subspace representation under our general assumptions can not be much computationally cheaper than solving the corresponding ℓ0 problem of ℓ0-SSC. We develop a novel approximate algorithm named Approximate ℓ0 -SSC (Aℓ0-SSC) that employs proximal gradient descent to obtain a sub-optimal solution to the optimization problem of ℓ0 -SSC with theoretical guarantee, and the sub-optimal solution is used to build a sparse similarity matrix for clustering. Extensive experimental results on various data sets demonstrate the superiority of Aℓ0-SSC compared to other competing clustering methods.</p><p></p><p><i><b>Keywords</b></i><i>: Sparse subspace clustering, proximal gradient descent</i></p>`,
    authors:
      "Yingzhen Yang, Jiashi Feng, Nebojsa Jojic, Jianchao Yang, Thomas S Huang",
    eventID: 1,
    researchArea: "Computer Vision",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/4NhUi3IjWabP9ZStNB71yX/7ec5e05db529a988045b8dc88c4fc15c/l0-ssc.pdf",
    metaDescription: `Subspace clustering methods with sparsity prior, such as Sparse Subspace Clustering (SSC), are effective in partitioning the data that lie in a union of subspaces. Most of those methods require certain assumptions, e.g. independence or disjointness, on the subspaces. These assumptions are not guaranteed to hold in practice and they limit the application of existing sparse subspace clustering methods. In this paper, we propose ℓ0-induced sparse subspace clustering (ℓ0-SSC). In contrast to the required assumptions, such as independence or disjointness, on subspaces for most existing sparse subspace clustering methods, we prove that subspace-sparse representation, a key element in subspace clustering, can be obtained by ℓ0-SSC for arbitrary distinct underlying subspaces almost surely under the mild i.i.d. assumption on the data generation. We also present the “no free lunch” theorem that obtaining the subspace representation under our general assumptions can not be much computationally cheaper than solving the corresponding ℓ0 problem of ℓ0-SSC. We develop a novel approximate algorithm named Approximate ℓ0 -SSC (Aℓ0-SSC) that employs proximal gradient descent to obtain a sub-optimal solution to the optimization problem of ℓ0 -SSC with theoretical guarantee, and the sub-optimal solution is used to build a sparse similarity matrix for clustering. Extensive experimental results on various data sets demonstrate the superiority of Aℓ0-SSC compared to other competing clustering methods`,
    bgImage: "",
  },
  {
    id: 2,
    date: "February 25, 2016",
    title:
      "Cross-modality consistent regression for joint visual-textual sentiment analysis of social multimedia",
    description: `<p><b>Abstract: </b>Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using additional images and videos to express their opinions and share their experiences. Sentiment analysis of such large-scale textual and visual content can help better extract user sentiments toward events or topics. Motivated by the needs to leverage large-scale social multimedia content for sentiment analysis, we propose a cross-modality consistent regression (CCR) model, which is able to utilize both the state-of-the-art visual and textual sentiment analysis techniques. We first fine-tune a convolutional neural network (CNN) for image sentiment analysis and train a paragraph vector model for textual sentiment analysis. On top of them, we train our multi-modality regression model. We use sentimental queries to obtain half a million training samples from Getty Images. We have conducted extensive experiments on both machine weakly labeled and manually labeled image tweets. The results show that the proposed model can achieve better performance than the state-of-the art textual and visual sentiment analysis algorithms alone.</p>`,
    authors: "Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang",
    eventID: 2,
    researchArea: "Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/tkDiTZmn08rmPZFkMJTUu/32ec515009d01b109b8ca619e57d8d43/Cross-modality_Consistent_Regression_for_Joint_Visual-Textual_Sentiment_Analysis_of_Social_Multimedia.pdf",
    metaDescription: `Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using additional images and videos to express their opinions and share their experiences. Sentiment analysis of such large-scale textual and visual content can help better extract user sentiments toward events or topics. Motivated by the needs to leverage large-scale social multimedia content for sentiment analysis, we propose a cross-modality consistent regression (CCR) model, which is able to utilize both the state-of-the-art visual and textual sentiment analysis techniques. We first fine-tune a convolutional neural network (CNN) for image sentiment analysis and train a paragraph vector model for textual sentiment analysis. On top of them, we train our multi-modality regression model. We use sentimental queries to obtain half a million training samples from Getty Images. We have conducted extensive experiments on both machine weakly labeled and manually labeled image tweets. The results show that the proposed model can achieve better performance than the state-of-the art textual and visual sentiment analysis algorithms alone.`,
    bgImage: "",
  },
  {
    id: 3,
    date: "May 09, 2016",
    title:
      "Building a large scale dataset for image emotion recognition: the fine print and the benchmark",
    description: `<p><b>Abstract: </b></p><p>Psychological research results have confirmed that people can have different emotional reactions to different visual stimuli. Several papers have been published on the problem of visual emotion analysis. In particular, attempts have been made to analyze and predict people’s emotional reaction towards images. To this end, different kinds of hand-tuned features are proposed. The results reported on several carefully selected and labeled small image data sets have confirmed the promise of such features. While the recent successes of many computer vision related tasks are due to the adoption of Convolutional Neural Networks (CNNs), visual emotion analysis has not achieved the same level of success. This may be primarily due to the unavailability of confidently labeled and relatively large image data sets for visual emotion analysis. In this work, we introduce a new data set, which started from 3+ million weakly labeled images of different emotions and ended up 30 times as large as the current largest publicly available visual emotion data set. We hope that this data set encourages further research on visual emotion analysis. We also perform extensive benchmarking analyses on this large data set using the state of the art methods including CNNs.</p>`,
    authors: "Quanzeng You and Jiebo Luo, Hailin Jin, Jianchao Yang",
    eventID: 3,
    researchArea: "Computer Vision",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/6WA4ni9kd2aBfgHkAFpmst/a58ae6385cfad8dfd0b104e8cdc22192/1605.02677.pdf",
    metaDescription: `Psychological research results have confirmed that people can have different emotional reactions to different visual stimuli. Several papers have been published on the problem of visual emotion analysis. In particular, attempts have been made to analyze and predict people’s emotional reaction towards images. To this end, different kinds of hand-tuned features are proposed. The results reported on several carefully selected and labeled small image data sets have confirmed the promise of such features. While the recent successes of many computer vision related tasks are due to the adoption of Convolutional Neural Networks (CNNs), visual emotion analysis has not achieved the same level of success. This may be primarily due to the unavailability of confidently labeled and relatively large image data sets for visual emotion analysis. In this work, we introduce a new data set, which started from 3+ million weakly labeled images of different emotions and ended up 30 times as large as the current largest publicly available visual emotion data set. We hope that this data set encourages further research on visual emotion analysis. We also perform extensive benchmarking analyses on this large data set using the state of the art methods including CNNs.`,
    bgImage: "",
  },
  {
    id: 4,
    date: "July 18, 2016",
    title:
      "Deep edge guided recurrent residual learning for image super-resolution",
    description: `<p><b>Abstract:</b> In this work, we consider the image super-resolution (SR) problem. The main challenge of image SR is to recover high-frequency details of a low-resolution (LR) image that are important for human perception. To address this essentially ill-posed problem, we introduce a Deep Edge Guided REcurrent rEsidual (DEGREE) network to progressively recover the high-frequency details. Different from most of existing methods that aim at predicting high-resolution (HR) images directly, DEGREE investigates an alternative route to recover the difference between a pair of LR and HR images by recurrent residual learning. DEGREE further augments the SR process with edge-preserving capability, namely the LR image and its edge map can jointly infer the sharp edge details of the HR image during the recurrent recovery process. To speed up its training convergence rate, by-pass connections across multiple layers of DEGREE are constructed. In addition, we offer an understanding on DEGREE from the view-point of sub-band frequency decomposition on image signal and experimentally demonstrate how DEGREE can recover different frequency bands separately. Extensive experiments on three benchmark datasets clearly demonstrate the superiority of DEGREE over well-established baselines and DEGREE also provides new state-of-the-arts on these datasets.</p>`,
    authors:
      "Wenhan Yang, Jiashi Feng, Jianchao Yang, Fang Zhao, Jiaying Liu, Zongming Guo, Shuicheng Yan",
    eventID: 4,
    researchArea: "CoDeep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/6UwtxaTlwopnnoMlObPHW7/d6e84299505ede3036c7c282ac48e205/1604.08671.pdf",
    metaDescription: `In this work, we consider the image super-resolution (SR) problem. The main challenge of image SR is to recover high-frequency details of a low-resolution (LR) image that are important for human perception. To address this essentially ill-posed problem, we introduce a Deep Edge Guided REcurrent rEsidual (DEGREE) network to progressively recover the high-frequency details. Different from most of existing methods that aim at predicting high-resolution (HR) images directly, DEGREE investigates an alternative route to recover the difference between a pair of LR and HR images by recurrent residual learning. DEGREE further augments the SR process with edge-preserving capability, namely the LR image and its edge map can jointly infer the sharp edge details of the HR image during the recurrent recovery process. To speed up its training convergence rate, by-pass connections across multiple layers of DEGREE are constructed. In addition, we offer an understanding on DEGREE from the view-point of sub-band frequency decomposition on image signal and experimentally demonstrate how DEGREE can recover different frequency bands separately. Extensive experiments on three benchmark datasets clearly demonstrate the superiority of DEGREE over well-established baselines and DEGREE also provides new state-of-the-arts on these datasets.`,
    bgImage: "",
  },
  {
    id: 5,
    date: "August 23, 2016",
    title: "A Recurrent Encoder-Decoder Network for Sequential Face Alignment",
    description: `<p>We propose a novel recurrent encoder-decoder network model for real-time video-based face alignment. Our proposed model predicts 2D facial point maps regularized by a regression loss, while uniquely exploiting recurrent learning at both spatial and temporal dimensions. At the spatial level, we add a feedback loop connection between the combined output response map and the input, in order to enable iterative coarse-to-fine face alignment using a single network model. At the temporal level, we first decouple the features in the bottleneck of the network into temporalvariant factors, such as pose and expression, and temporalinvariant factors, such as identity information. Temporal recurrent learning is then applied to the decoupled temporalvariant features, yielding better generalization and significantly more accurate results at test time. We perform a comprehensive experimental analysis, showing the importance of each component of our proposed model, as well as superior results over the state-of-the-art in standard datasets.</p>`,
    authors: "Xi Peng, Rogerio Feris, Xiaoyu Wang, Dimitris Metaxas",
    eventID: 5,
    researchArea: "Computer Vision, Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/3JlSlyXMHIFvYcgtx7ZwB0/ee9bd42c1b5f4d4a57dab79f91dff9d3/ECCV16.pdf",
    metaDescription: `We propose a novel recurrent encoder-decoder network model for real-time video-based face alignment. Our proposed model predicts 2D facial point maps regularized by a regression loss, while uniquely exploiting recurrent learning at both spatial and temporal dimensions. At the spatial level, we add a feedback loop connection between the combined output response map and the input, in order to enable iterative coarse-to-fine face alignment using a single network model. At the temporal level, we first decouple the features in the bottleneck of the network into temporalvariant factors, such as pose and expression, and temporalinvariant factors, such as identity information. Temporal recurrent learning is then applied to the decoupled temporalvariant features, yielding better generalization and significantly more accurate results at test time. We perform a comprehensive experimental analysis, showing the importance of each component of our proposed model, as well as superior results over the state-of-the-art in standard datasets.`,
    bgImage: "",
  },
  {
    id: 6,
    date: "October 01, 2016",
    title: "Deep Networks for Image Super-Resolution with Sparse Prior",
    description: `<p>Deep learning techniques have been successfully applied in many areas of computer vision, including low-level image restoration problems. For image super-resolution, several models based on deep neural networks have been recently proposed and attained superior performance that overshadows all previous handcrafted models. The question then arises whether large-capacity and data-driven models have become the dominant solution to the ill-posed super resolution problem. In this paper, we argue that domain expertise represented by the conventional sparse coding model is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end. The interpretation of the network based on sparse coding leads to much more efficient and effective training, as well as a reduced model size. Our model is evaluated on a wide range of images, and shows clear advantage over existing state-of-the-art methods in terms of both restoration accuracy and human subjective quality.</p>`,
    authors: "Zhaowen Wang, Ding Lu, Jianchao Yang, Wei Han, Thomas Huang",
    eventID: 6,
    researchArea: "Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/uUttPbOk96b5a769kLzFG/8d75c1d7749cbe547b27b434ac8fa636/Wang_Deep_Networks_for_ICCV_2015_paper.pdf",
    metaDescription: `Deep learning techniques have been successfully applied in many areas of computer vision, including low-level image restoration problems. For image super-resolution, several models based on deep neural networks have been recently proposed and attained superior performance that overshadows all previous handcrafted models. The question then arises whether large-capacity and data-driven models have become the dominant solution to the ill-posed super resolution problem. In this paper, we argue that domain expertise represented by the conventional sparse coding model is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end. The interpretation of the network based on sparse coding leads to much more efficient and effective training, as well as a reduced model size. Our model is evaluated on a wide range of images, and shows clear advantage over existing state-of-the-art methods in terms of both restoration accuracy and human subjective quality.`,
    bgImage: "",
  },
  {
    id: 7,
    date: "November 17, 2016",
    title: "AutoScaler: Scale-Attention Networks for Visual Correspondence",
    description: `<p><b>Abstract</b>: Finding visual correspondence between local features is key to many computer vision problems. While defining features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose AutoScaler, a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our network consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive receptive field sizes over different scales of the input. The entire network is trained end-to-end in a siamese framework for visual correspondence tasks. Our method achieves favorable results compared to state-of-the-art methods on challenging optical flow and semantic matching benchmarks, including Sintel, KITTI and CUB2011. We also show that our method can generalize to improve hand-crafted descriptors (e.g Daisy) on general visual correspondence tasks. Finally, our attention network can generate visually interpretable scale attention maps.</p>`,
    authors: "Shenlong Wang, Linjie Luo, Ning Zhang, Jia Li",
    eventID: 7,
    researchArea: "Computer Vision, Computer Graphics",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/17QoFss6QsgPILBQV69tEI/0d4f114dafc0f5e9731ad186e477cacd/1611.05837.pdf",
    metaDescription: `Finding visual correspondence between local features is key to many computer vision problems. While defining features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose AutoScaler, a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our network consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive receptive field sizes over different scales of the input. The entire network is trained end-to-end in a siamese framework for visual correspondence tasks. Our method achieves favorable results compared to state-of-the-art methods on challenging optical flow and semantic matching benchmarks, including Sintel, KITTI and CUB2011. We also show that our method can generalize to improve hand-crafted descriptors (e.g Daisy) on general visual correspondence tasks. Finally, our attention network can generate visually interpretable scale attention maps.`,
    bgImage: "",
  },
  {
    id: 8,
    date: "March 07, 2017",
    title: "Learning from noisy labels with distillation",
    description: `<p><b>Abstract: </b>The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, the label noises have been treated as statistical outliers, and approaches such as importance re-weighting and bootstrap have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multi-mode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use side information, including a small clean dataset and label relations in knowledge graph, to "hedge the risk" of learning from noisy labels. Furthermore, unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains.</p>`,
    authors:
      "Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, Jia Li",
    eventID: 8,
    researchArea: "Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/2A1ODqDA7z54d3zOSBDIw0/930c86f9dbb56508d73fc306b2527fa2/1703.02391.pdf",
    metaDescription: `The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, the label noises have been treated as statistical outliers, and approaches such as importance re-weighting and bootstrap have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multi-mode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use side information, including a small clean dataset and label relations in knowledge graph, to "hedge the risk" of learning from noisy labels. Furthermore, unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains.`,
    bgImage: "",
  },
  {
    id: 9,
    date: "April 12, 2017",
    title:
      "Deep Reinforcement Learning-based Image Captioning with Embedding Reward",
    description: `<p><b>Abstract: </b>Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a “policy network” and a “value network” to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-ofthe art approaches across different evaluation metrics.</p>`,
    authors: "Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, Jia Li",
    eventID: 9,
    researchArea: "Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/1n7nYPrqsTuoayWdCdRJb2/b4e224b9df519f4f85faa81e0b8720f0/1704.03899.pdf",
    metadescription: `Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a “policy network” and a “value network” to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-ofthe art approaches across different evaluation metrics.`,
    bgImage: "",
  },
  {
    id: 10,
    date: "April 13, 2017",
    title: "Support Regularized Sparse Coding and Its Fast Encoder",
    description: `<p><b>Abstract: </b>Sparse coding represents a signal by a linear combination of only a few atoms of a learned over-complete dictionary. While sparse coding exhibits compelling performance for various machine learning tasks, the process of obtaining sparse code with fixed dictionary is independent for each data point without considering the geometric information and manifold structure of the entire data. We propose Support Regularized Sparse Coding (SRSC) which produces sparse codes that account for the manifold structure of the data by encouraging nearby data in the manifold to choose similar dictionary atoms. In this way, the obtained support regularized sparse codes capture the locally linear structure of the data manifold and enjoy robustness to data noise. We present the optimization algorithm of SRSC with theoretical guarantee for the optimization over the sparse codes. We also propose a feed-forward neural network termed Deep Support Regularized Sparse Coding (Deep-SRSC) as a fast encoder to approximate the sparse codes generated by SRSC. Extensive experimental results demonstrate the effectiveness of SRSC and Deep-SRSC.</p>`,
    authors:
      "Yingzhen Yang, Jiahui Yu, Pushmeet Kohli, Jianchao Yang, Thomas S. Huang",
    eventID: 10,
    researchArea: "Computer Vision, Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/2PRq2iqW3cXS8WV3C86OBm/4afa0ce4ade6fb46987620288f20e746/pdf",
    metadescription: `Sparse coding represents a signal by a linear combination of only a few atoms of a learned over-complete dictionary. While sparse coding exhibits compelling performance for various machine learning tasks, the process of obtaining sparse code with fixed dictionary is independent for each data point without considering the geometric information and manifold structure of the entire data. We propose Support Regularized Sparse Coding (SRSC) which produces sparse codes that account for the manifold structure of the data by encouraging nearby data in the manifold to choose similar dictionary atoms. In this way, the obtained support regularized sparse codes capture the locally linear structure of the data manifold and enjoy robustness to data noise. We present the optimization algorithm of SRSC with theoretical guarantee for the optimization over the sparse codes. We also propose a feed-forward neural network termed Deep Support Regularized Sparse Coding (Deep-SRSC) as a fast encoder to approximate the sparse codes generated by SRSC. Extensive experimental results demonstrate the effectiveness of SRSC and Deep-SRSC.`,
    bgImage: "",
  },
  {
    id: 11,
    date: "April 19, 2017",
    title: "Exploring Personalized Neural Conversational Models",
    description: `<p><b>Abstract: </b>Modeling dialog systems is currently one of the most active problems in Natural Language Processing. Recent advances in Deep Learning have sparked an interest in the use of neural networks in modeling language, particularly for personalized conversational agents that can retain contextual information during dialog exchanges. This work carefully explores and compares several of the recently proposed neural conversation models, and carries out a detailed evaluation on the multiple factors that can significantly affect predictive performance, such as pretraining, embedding training, data cleaning, diversity-based reranking, evaluation setting, etc. Based on the tradeoffs of different models, we propose a new neural generative dialog model conditioned on speakers as well as context history that outperforms previous models on both retrieval and generative metrics. Our findings indicate that pretraining speaker embeddings on larger datasets, as well as bootstrapping word and speaker embeddings, can significantly improve performance (up to 3 points in perplexity), and that promoting diversity in using Mutual Information based techniques has a very strong effect in ranking metrics.</p>`,
    authors: "Satwik Kottur, Xiaoyu Wang, Vitor Carvalho",
    eventID: 11,
    researchArea: "Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/5pU2OrscrgE9YzYhSmetyC/361fca637bde81e27ee737086a538ce7/0521.pdf",
    metadescription: `Modeling dialog systems is currently one of the most active problems in Natural Language Processing. Recent advances in Deep Learning have sparked an interest in the use of neural networks in modeling language, particularly for personalized conversational agents that can retain contextual information during dialog exchanges. This work carefully explores and compares several of the recently proposed neural conversation models, and carries out a detailed evaluation on the multiple factors that can significantly affect predictive performance, such as pretraining, embedding training, data cleaning, diversity-based reranking, evaluation setting, etc. Based on the tradeoffs of different models, we propose a new neural generative dialog model conditioned on speakers as well as context history that outperforms previous models on both retrieval and generative metrics. Our findings indicate that pretraining speaker embeddings on larger datasets, as well as bootstrapping word and speaker embeddings, can significantly improve performance (up to 3 points in perplexity), and that promoting diversity in using Mutual Information based techniques has a very strong effect in ranking metrics.`,
    bgImage: "",
  },
  {
    id: 12,
    date: "August 07, 2017",
    title: "Dense Captioning with Joint Inference and Visual Context",
    description: `<p><b>Abstract:</b> Dense captioning is a newly emerging computer vision topic for understanding images with dense language descriptions. The goal is to densely detect visual concepts (e.g., objects, object parts, and interactions between them) from images, labeling each with a short descriptive phrase. We identify two key challenges of dense captioning that need to be properly addressed when tackling the problem. First, dense visual concept annotations in each image are associated with highly overlapping target regions, making accurate localization of each visual concept challenging. Second, the large amount of visual concepts makes it hard to recognize each of them by appearance alone. We propose a new model pipeline based on two novel ideas, joint inference and context fusion, to alleviate these two challenges. We design our model architecture in a methodical manner and thoroughly evaluate the variations in architecture. Our final model, compact and efficient, achieves stateof-the-art accuracy on Visual Genome [23] for dense captioning with a relative gain of 73% compared to the previous best algorithm. Qualitative experiments also reveal the semantic capabilities of our model in dense captioning. Our code is released at https://github.com/linjieyangsc/densecap.</p>`,
    authors: "Linjie Yang, Kevin Tang, Jianchao Yang, Jia Li",
    eventID: 12,
    researchArea: "Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/7gshHuh5hdciewRMtyyOxk/d560a01fb01759e877083a3413aefc3d/1611.06949.pdf",
    metadescription: `Dense captioning is a newly emerging computer vision topic for understanding images with dense language descriptions. The goal is to densely detect visual concepts (e.g., objects, object parts, and interactions between them) from images, labeling each with a short descriptive phrase. We identify two key challenges of dense captioning that need to be properly addressed when tackling the problem. First, dense visual concept annotations in each image are associated with highly overlapping target regions, making accurate localization of each visual concept challenging. Second, the large amount of visual concepts makes it hard to recognize each of them by appearance alone. We propose a new model pipeline based on two novel ideas, joint inference and context fusion, to alleviate these two challenges. We design our model architecture in a methodical manner and thoroughly evaluate the variations in architecture. Our final model, compact and efficient, achieves stateof-the-art accuracy on Visual Genome [23] for dense captioning with a relative gain of 73% compared to the previous best algorithm. Qualitative experiments also reveal the semantic capabilities of our model in dense captioning. Our code is released at https://github.com/linjieyangsc/densecap.`,
    bgImage: "",
  },
  {
    id: 13,
    date: "August 12, 2017",
    title: "Neighborhood regularized L1-graph",
    description: `<p><b>Abstract:</b>
    L1-Graph, which learns a sparse graph over the data by sparse representation, has been demonstrated to be effective in clustering especially for high dimensional data. Although it achieves compelling performance, the sparse graph generated by L1-Graph ignores the geometric information of the data by sparse representation for each datum separately. To obtain a sparse graph that is aligned to the underlying manifold structure of the data, we propose the novel Neighborhood Regularized L1-Graph (NRL1-Graph). NRL1-Graph learns sparse graph with locally consistent neighborhood by encouraging nearby data to have similar neighbors in the constructed sparse graph. We present the optimization algorithm of NRL1-Graph with theoretical guarantee on the convergence and the gap between the suboptimal solution and the globally optimal solution in each step of the coordinate descent, which is essential for the overall optimization of NRL1-Graph. Its provable accelerated version, NRL1 -Graph by Random Projection (NRL1-Graph-RP) that employs randomized data matrix decomposition, is also presented to improve the efficiency of the optimization of NRL1-Graph. Experimental results on various real data sets demonstrate the effectiveness of both NRL1-Graph and NRL1-Graph-RP.</p>`,
    authors:
      "Yingzhen Yang, Jiashi Feng, Jiahui Yu, Jianchao Yang, Pushmeet Kohli, Thomas S. Huang",
    eventID: 13,
    researchArea: "Computer Vision",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/61kzxlUbmEXr7HQP2GABdb/0735af652127cbe1c4d8df3873bb2627/244.pdf",
    metadescription: `L1-Graph, which learns a sparse graph over the data by sparse representation, has been demonstrated to be effective in clustering especially for high dimensional data. Although it achieves compelling performance, the sparse graph generated by L1-Graph ignores the geometric information of the data by sparse representation for each datum separately. To obtain a sparse graph that is aligned to the underlying manifold structure of the data, we propose the novel Neighborhood Regularized L1-Graph (NRL1-Graph). NRL1-Graph learns sparse graph with locally consistent neighborhood by encouraging nearby data to have similar neighbors in the constructed sparse graph. We present the optimization algorithm of NRL1-Graph with theoretical guarantee on the convergence and the gap between the suboptimal solution and the globally optimal solution in each step of the coordinate descent, which is essential for the overall optimization of NRL1-Graph. Its provable accelerated version, NRL1 -Graph by Random Projection (NRL1-Graph-RP) that employs randomized data matrix decomposition, is also presented to improve the efficiency of the optimization of NRL1-Graph. Experimental results on various real data sets demonstrate the effectiveness of both NRL1-Graph and NRL1-Graph-RP.`,
    bgImage: "",
  },
  {
    id: 14,
    date: "August 20, 2017",
    title:
      "Attention based CLDNNs for short-duration acoustic scene classification",
    description: `<p>Recently, neural networks with deep architecture have been widely applied to acoustic scene classification. Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory Networks (LSTMs) have shown improvements over fully connected Deep Neural Networks (DNNs). Motivated by the fact that CNNs, LSTMs and DNNs are complimentary in their modeling capability, we apply the CLDNNs (Convolutional, Long Short-Term Memory, Deep Neural Networks) framework to short-duration acoustic scene classification in a unified architecture. The CLDNNs take advantage of frequency modeling with CNNs, temporal modeling with LSTM, and discriminative training with DNNs. Based on the CLDNN architecture, several novel attention-based mechanisms are proposed and applied on the LSTM layer to predict the importance of each time step. We evaluate the proposed method on the truncated version of the 2016 TUT acoustic scenes dataset which consists of recordings from 15 different scenes. By using CLDNNs with bidirectional LSTM, we achieve higher performance compared to the conventional neural network architectures. Moreover, by combining the attention-weighted output with LSTM final time step output, significant improvement can be further achieved.</p>`,
    authors: "Jinxi Buo, Ning Xu, Jia Li, Abeer Alwan",
    eventID: 14,
    researchArea: "Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/45Cb9LqqbBMb8qe0EiLFji/a36e8966b08c79a252b8084f6900c68d/Jinxi-IS17-audio-scene.pdf",
    metadescription: `Recently, neural networks with deep architecture have been widely applied to acoustic scene classification. Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory Networks (LSTMs) have shown improvements over fully connected Deep Neural Networks (DNNs). Motivated by the fact that CNNs, LSTMs and DNNs are complimentary in their modeling capability, we apply the CLDNNs (Convolutional, Long Short-Term Memory, Deep Neural Networks) framework to short-duration acoustic scene classification in a unified architecture. The CLDNNs take advantage of frequency modeling with CNNs, temporal modeling with LSTM, and discriminative training with DNNs. Based on the CLDNN architecture, several novel attention-based mechanisms are proposed and applied on the LSTM layer to predict the importance of each time step. We evaluate the proposed method on the truncated version of the 2016 TUT acoustic scenes dataset which consists of recordings from 15 different scenes. By using CLDNNs with bidirectional LSTM, we achieve higher performance compared to the conventional neural network architectures. Moreover, by combining the attention-weighted output with LSTM final time step output, significant improvement can be further achieved.`,
    bgImage: "",
  },
  {
    id: 15,
    date: "September 04, 2017",
    title: "Multiple Instance Visual-Semantic Embedding",
    description: `<p><b>Abstract: </b>Visual-semantic embedding models have been recently proposed and shown to be effective for image classification and zero-shot learning. The key idea is that by directly learning a mapping from images into a semantic label space, the algorithm can generalize to a large number of unseen labels. However, existing approaches are limited to single-label embedding, handling images with multiple labels still remains an open problem, mainly due to the complex underlying correspondence between an image and its labels. In this work, we present a novel Multiple Instance Visual-Semantic Embedding (MIVSE) model for multi-label images. Instead of embedding a whole image into the semantic space, our model characterizes the subregion-to-label correspondence, which discovers and maps semantically meaningful image subregions to the corresponding labels. Experiments on two challenging tasks, multi-label image annotation and zero-shot learning, show that the proposed MIVSE model outperforms state-of-the-art methods on both tasks and possesses the ability of generalizing to unseen labels.</p>`,
    authors: "Zhou Ren, Hailin Jin, Zhe Lin, Chen Fang, Alan Yuille",
    eventID: 15,
    researchArea: "Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/3Rn4cNWMuzkZyxie5UUyrZ/2c424ccd65277258717360edf0232a6c/Zhou_bmvc17_paper.pdf",
    metadescription: `Visual-semantic embedding models have been recently proposed and shown to be effective for image classification and zero-shot learning. The key idea is that by directly learning a mapping from images into a semantic label space, the algorithm can generalize to a large number of unseen labels. However, existing approaches are limited to single-label embedding, handling images with multiple labels still remains an open problem, mainly due to the complex underlying correspondence between an image and its labels. In this work, we present a novel Multiple Instance Visual-Semantic Embedding (MIVSE) model for multi-label images. Instead of embedding a whole image into the semantic space, our model characterizes the subregion-to-label correspondence, which discovers and maps semantically meaningful image subregions to the corresponding labels. Experiments on two challenging tasks, multi-label image annotation and zero-shot learning, show that the proposed MIVSE model outperforms state-of-the-art methods on both tasks and possesses the ability of generalizing to unseen labels.`,
    bgImage: "",
  },
  {
    id: 16,
    date: "September 05, 2017",
    title:
      "On the Sub-Optimality of Proximal Gradient Descent for L0 Sparse Approximation",
    description: `<p><b>Abstract: </b></p><p>We study the proximal gradient descent (PGD) method for ℓ0 sparse approximation problem as well as its accelerated optimization with randomized algorithms in this paper. We first offer theoretical analysis of PGD showing the bounded gap between the sub-optimal solution by PGD and the globally optimal solution for the ℓ0 sparse approximation problem under conditions weaker than Restricted Isometry Property widely used in compressive sensing literature. Moreover, we propose randomized algorithms to accelerate the optimization by PGD using randomized low rank matrix approximation (PGD-RMA) and randomized dimension reduction (PGD-RDR). Our randomized algorithms substantially reduces the computation cost of the original PGD for the ℓ0 sparse approximation problem, and the resultant sub-optimal solution still enjoys provable suboptimality, namely, the sub-optimal solution to the reduced problem still has bounded gap to the globally optimal solution to the original problem.</p>`,
    authors: "Yingzhen Yang, Jianchao Yang, Wei Han, Thomas. S. Huang",
    eventID: 16,
    researchArea: "Computer Vision",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/Ka0rDPWzW9ogzuqb1xAE3/1f34ab5b563c10dc57b70c937b1eb93b/1709.01230.pdf",
    metadescription: `We study the proximal gradient descent (PGD) method for ℓ0 sparse approximation problem as well as its accelerated optimization with randomized algorithms in this paper. We first offer theoretical analysis of PGD showing the bounded gap between the sub-optimal solution by PGD and the globally optimal solution for the ℓ0 sparse approximation problem under conditions weaker than Restricted Isometry Property widely used in compressive sensing literature. Moreover, we propose randomized algorithms to accelerate the optimization by PGD using randomized low rank matrix approximation (PGD-RMA) and randomized dimension reduction (PGD-RDR). Our randomized algorithms substantially reduces the computation cost of the original PGD for the ℓ0 sparse approximation problem, and the resultant sub-optimal solution still enjoys provable suboptimality, namely, the sub-optimal solution to the reduced problem still has bounded gap to the globally optimal solution to the original problem.`,
    bgImage: "",
  },
  {
    id: 17,
    date: "September 11, 2017",
    title: "Viewpoint-Consistent 3D Face Alignment",
    description: `<p><b>Abstract:</b> Most approaches to face alignment treat the face as a 2D object, which fails to represent depth variation and is vulnerable to loss of shape consistency when the face rotates along a 3D axis. Because faces commonly rotate three dimensionally, 2D approaches are vulnerable to significant error. 3D morphable models, employed as a second step in 2D+3D approaches are robust to face rotation but are computationally too expensive for many applications, yet their ability to maintain viewpoint consistency is unknown. We present an alternative approach that estimates 3D face landmarks in a single face image. The method uses a regression forest-based algorithm that
    adds a third dimension to the common cascade pipeline. 3D face landmarks are estimated directly, which avoids fitting a 3D morphable model.The proposed method achieves viewpoint consistency in a computationally efficient manner that is robust to 3D face rotation. To train and test our approach, we introduce the Multi-PIE Viewpoint Consistent database. In empirical tests, the proposed method achieved simple yet effective head pose estimation and viewpoint consistency on multiple measures relative to alternative approaches.</p>`,
    authors: "Sergey Tulyakov, László A. Jeni, Jeffrey F. Cohn, Nicu Sebe",
    eventID: 17,
    researchArea: "Computer Vision",
    PDFLink:
      "https://downloads.ctfassets.net/btheynltg5cn/2U4FFnfJmyPsEZIyHsp2aR/da29065b102e918f67f77711b4bb6606/Tulyakov17TPAMI_Multiview.pdf",
    metadescription: `Most approaches to face alignment treat the face as a 2D object, which fails to represent depth variation and is vulnerable to loss of shape consistency when the face rotates along a 3D axis. Because faces commonly rotate three dimensionally, 2D approaches are vulnerable to significant error. 3D morphable models, employed as a second step in 2D+3D approaches are robust to face rotation but are computationally too expensive for many applications, yet their ability to maintain viewpoint consistency is unknown. We present an alternative approach that estimates 3D face landmarks in a single face image. The method uses a regression forest-based algorithm that adds a third dimension to the common cascade pipeline. 3D face landmarks are estimated directly, which avoids fitting a 3D morphable model.The proposed method achieves viewpoint consistency in a computationally efficient manner that is robust to 3D face rotation. To train and test our approach, we introduce the Multi-PIE Viewpoint Consistent database. In empirical tests, the proposed method achieved simple yet effective head pose estimation and viewpoint consistency on multiple measures relative to alternative approaches.`,
    bgImage: "",
  },
  {
    id: 18,
    date: "November 01, 2017",
    title: "Tensor Field Design in Volumes",
    description: `<p><b>Abstract: </b>3D tensor field design is important in several graphics applications such as procedural noise, solid texturing, and geometry synthesis. Different fields can lead to different visual effects. The topology of a tensor field, such as degenerate tensors, can cause artifacts in these applications. Existing 2D tensor field design systems cannot be used to handle the topology of a 3D tensor field. In this paper, we present to our knowledge the first 3D tensor field design system. At the core of our system is the ability to edit the topology of tensor fields. We demonstrate the power of our design system with applications in solid texturing and geometry synthesis.</p>`,
    authors:
      "Jonathan Palacios, Lawrence Roy, Prashant Kumar, Chen-Yuan Hsu, Weikai Chen, Chongyang Ma, Li-Yi Wei, Eugene Zhang",
    eventID: 18,
    researchArea: "Computer Graphics",
    PDFLink:
      "https://downloads.ctfassets.net/btheynltg5cn/7rJDOnLj3qW3S9T6EMjVzT/fee3cd0983615ea0b69ad3373f5472f4/2017_tf_preprint.pdf",
    metadescription: `3D tensor field design is important in several graphics applications such as procedural noise, solid texturing, and geometry synthesis. Different fields can lead to different visual effects. The topology of a tensor field, such as degenerate tensors, can cause artifacts in these applications. Existing 2D tensor field design systems cannot be used to handle the topology of a 3D tensor field. In this paper, we present to our knowledge the first 3D tensor field design system. At the core of our system is the ability to edit the topology of tensor fields. We demonstrate the power of our design system with applications in solid texturing and geometry synthesis.`,
    bgImage: "",
  },
  {
    id: 19,
    date: "November 30, 2017",
    title:
      "Hybrid-VAE: Improving Deep Generative Models by Partial Observations",
    description: `<p><b>Abstract: </b>Deep neural network models trained on large labeled datasets are the state-of-theart in a large variety of computer vision tasks. In many applications, however, labeled data is expensive to obtain or requires a time consuming manual annotation process. In contrast, unlabeled data is often abundant and available in large quantities. We present a principled framework to capitalize on unlabeled data by training deep generative models on both labeled and unlabeled data. We show that such a combination is beneficial because the unlabeled data acts as a data-driven form of regularization, allowing generative models trained on few labeled samples to reach the performance of fully-supervised generative models trained on much larger datasets. We call our method Hybrid VAE (H-VAE) as it contains both the generative and the discriminative parts. We validate H-VAE on three large-scale datasets of different modalities: two face datasets: (MultiPIE, CelebA) and a hand pose dataset (NYU Hand Pose). Our qualitative visualizations further support improvements achieved by using partial observations.</p>`,
    authors: "Sergey Tulyakov, Andrew Fitzgibbon, Sebastian Nowozin",
    eventID: 19,
    researchArea: "Deep Learning",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/5RZIx7QcnFsJuNHCTU56GY/607f115e43acbeacc9662e39317e145c/1711.11566.pdf",
    metadescription: `Deep neural network models trained on large labeled datasets are the state-of-theart in a large variety of computer vision tasks. In many applications, however, labeled data is expensive to obtain or requires a time consuming manual annotation process. In contrast, unlabeled data is often abundant and available in large quantities. We present a principled framework to capitalize on unlabeled data by training deep generative models on both labeled and unlabeled data. We show that such a combination is beneficial because the unlabeled data acts as a data-driven form of regularization, allowing generative models trained on few labeled samples to reach the performance of fully-supervised generative models trained on much larger datasets. We call our method Hybrid VAE (H-VAE) as it contains both the generative and the discriminative parts. We validate H-VAE on three large-scale datasets of different modalities: two face datasets: (MultiPIE, CelebA) and a hand pose dataset (NYU Hand Pose). Our qualitative visualizations further support improvements achieved by using partial observations.`,
    bgImage: "",
  },
  {
    id: 20,
    date: "December 04, 2017",
    title:
      "A Graph Theory Approach To QP Problem Reformulation: An Example With SVM",
    description: `<p>Quadratic programming (QP) problem reformulations have been studied for decades
    [Sherali and Tuncbilek(1995), Nemirovski and Shapiro(2006), Anstreicher(2009)],
    [Zheng et al.(2012)Zheng, Sun, Li, and Cui, Wu and Jiang(2017)], but rarely linked to
    Graph Theory. Indeed, typical reformulations focus on convexifying a non-convex QP
    problem, making the objective function differentiable, optimizing on the continuous
    domain while ensuring the final solution is binary, or adding regularizers and Lagrangian
    coefficients to optimize the dual problem. In this paper, we take SVM as an example
    to demonstrate that QP problems can also be reformulated using the same mechanism
    as P/NP problem reduction, overcoming speed and memory footprint limitations from
    other types of reformulation. We show that SVM is comparable to a soft weighted edge
    independent set problem where the amount of support vectors per class is balanced, thus it can also be reformulated as a maximum weighted clique problem (MWC) with the same
    class balancing constraint. After adapting the sequential minimal optimization (SMO)
    algorithm [Platt(1998), Fan et al.(2005)Fan, Chen, and Lin] to our new MWC formulation,
    we demonstrate that such reformulation leads to improved performance (7∼36 time faster to train, and sparser solution for comparable accuracy).</p>`,
    authors: "William Brendel, Luis Marujo",
    eventID: 20,
    researchArea: "Data Mining",
    PDFLink:
      "https://assets.ctfassets.net/btheynltg5cn/3o0Zonc9K8N3i7iUB4ch43/9207663b99977a892807bf0ba2828da5/OPT2017_paper_11.pdf",
    metadescription: `Quadratic programming (QP) problem reformulations have been studied for decades [Sherali and Tuncbilek(1995), Nemirovski and Shapiro(2006), Anstreicher(2009)], [Zheng et al.(2012)Zheng, Sun, Li, and Cui, Wu and Jiang(2017)], but rarely linked to Graph Theory. Indeed, typical reformulations focus on convexifying a non-convex QP problem, making the objective function differentiable, optimizing on the continuous domain while ensuring the final solution is binary, or adding regularizers and Lagrangian coefficients to optimize the dual problem. In this paper, we take SVM as an example to demonstrate that QP problems can also be reformulated using the same mechanism as P/NP problem reduction, overcoming speed and memory footprint limitations from other types of reformulation. We show that SVM is comparable to a soft weighted edge independent set problem where the amount of support vectors per class is balanced, thus it can also be reformulated as a maximum weighted clique problem (MWC) with the same class balancing constraint. After adapting the sequential minimal optimization (SMO) algorithm [Platt(1998), Fan et al.(2005)Fan, Chen, and Lin] to our new MWC formulation, we demonstrate that such reformulation leads to improved performance (7∼36 time faster to train, and sparser solution for comparable accuracy).`,
    bgImage: "",
  },
  {
    id: 21,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 21,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 22,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 22,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 23,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 23,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 24,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 24,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 25,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 25,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 26,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 26,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 27,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 27,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 28,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 28,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 29,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 29,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 30,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 30,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 31,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 31,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 32,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 32,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 33,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 33,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 34,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 34,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 35,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 35,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 36,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 36,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 37,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 37,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 38,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 38,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 39,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 39,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 40,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 40,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 41,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 41,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 42,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 42,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 43,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 43,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 44,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 44,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 45,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 45,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 46,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 46,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 47,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 47,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 48,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 48,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 49,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 49,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 50,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 50,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 51,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 51,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 52,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 52,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 53,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 53,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 54,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 54,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 55,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 55,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 56,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 56,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 57,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 57,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 58,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 58,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 59,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 59,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 60,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 60,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 61,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 61,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 62,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 62,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 63,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 63,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 64,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 64,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 65,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 65,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 66,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 66,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 67,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 67,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 68,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 68,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 69,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 69,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 70,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 70,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 71,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 71,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 72,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 72,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 73,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 73,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 74,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 74,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 75,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 75,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 76,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 76,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 77,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 77,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 78,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 78,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 79,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 79,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 80,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 80,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 81,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 81,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 82,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 82,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 83,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 83,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 84,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 84,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 85,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 85,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 86,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 86,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 87,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 87,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 88,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 88,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 89,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 89,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 90,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 90,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 91,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 91,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 92,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 92,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 93,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 93,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 94,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 94,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 95,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 95,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 96,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 96,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 97,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 97,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 98,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 98,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
  {
    id: 99,
    date: "",
    title: "",
    description: ``,
    authors: "",
    eventID: 99,
    researchArea: "",
    PDFLink: "",
    metadescription: ``,
    bgImage: "",
  },
];

publicationsList.forEach((publication) => {
  publication.metaTitle = publication.title + " - Snap Research";
  publication.slug = publication.title.split(" ").join("-").toLocaleLowerCase();
  publication.url = "publications/publication.html#" + publication.slug;
  publication.link = "publications/publication.html";
});

publicationsList.sort((a, b) => new Date(b.date) - new Date(a.date));

function getPublicationsByIDs(id, path) {
  if (!id.length) return [];
  let result = [];
  id.map((insideID) => {
    publicationsList.map((item) => {
      if (item.id === insideID) {
        result.push(item);
      }
    });
  });
  return result
    .map((item) => {
      const tempObj = { ...item };
      tempObj.link = path + tempObj.link;
      return tempObj;
    })
    .sort((a, b) => new Date(b.date) - new Date(a.date));
}

function getPublicationBySlug(slug) {
  let result = publicationsList.filter((item) => slug.slice(1) === item.slug);
  return result;
}

publicationsList.map((obj) => {
  let result = eventsList.filter((event) => {
    return event.id === obj.eventID;
  });
  if (!result.length) return obj;
  return Object.assign(obj, { eventShort: result[0].eventShort });
});
